{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "class CustomEfficientNet(nn.Module):\n",
    "    def __init__(self, model_name=\"efficientnet-b0\", num_classes=6, freeze_backbone=False):\n",
    "        super(CustomEfficientNet, self).__init__()\n",
    "        # 사전학습된 EfficientNet 불러오기\n",
    "        self.efficientnet = EfficientNet.from_pretrained(model_name)\n",
    "        \n",
    "        # (옵션) 일부 레이어 동결\n",
    "        if freeze_backbone:\n",
    "            for param in self.efficientnet.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # EfficientNet에서 마지막 FC의 입력 차원 가져오기\n",
    "        in_features = self.efficientnet._fc.in_features\n",
    "        \n",
    "        # 새로운 분류기(FC) 레이어 정의\n",
    "        # 원하는 대로 Linear + Dropout + BatchNorm 등을 쌓을 수 있음\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),                  # Dropout 예시\n",
    "            nn.Linear(in_features, 256),        \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),                # BatchNorm1d 예시\n",
    "            nn.Dropout(p=0.3),                  # 추가 Dropout\n",
    "            nn.Linear(256, num_classes)         # 최종 출력층\n",
    "        )\n",
    "        \n",
    "        # 기존 EfficientNet의 분류 레이어를 우리가 정의한 classifier로 교체\n",
    "        # (EfficientNet이 _fc 속성을 통해 분류를 담당함)\n",
    "        self.efficientnet._fc = self.classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.efficientnet(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Epoch [1/20]\n",
      "  Train Loss: 1.2695 | Train Acc: 0.5282\n",
      "  Test  Loss: 1.3913 | Test  Acc: 0.4407\n",
      "Epoch [2/20]\n",
      "  Train Loss: 0.4020 | Train Acc: 0.8997\n",
      "  Test  Loss: 1.0705 | Test  Acc: 0.5706\n",
      "Epoch [3/20]\n",
      "  Train Loss: 0.1860 | Train Acc: 0.9619\n",
      "  Test  Loss: 0.7950 | Test  Acc: 0.7175\n",
      "Epoch [4/20]\n",
      "  Train Loss: 0.0770 | Train Acc: 0.9915\n",
      "  Test  Loss: 0.7790 | Test  Acc: 0.7514\n",
      "Epoch [5/20]\n",
      "  Train Loss: 0.0546 | Train Acc: 0.9929\n",
      "  Test  Loss: 0.8192 | Test  Acc: 0.7627\n",
      "Epoch [6/20]\n",
      "  Train Loss: 0.0522 | Train Acc: 0.9915\n",
      "  Test  Loss: 0.8735 | Test  Acc: 0.7684\n",
      "Epoch [7/20]\n",
      "  Train Loss: 0.0414 | Train Acc: 0.9929\n",
      "  Test  Loss: 0.9158 | Test  Acc: 0.7571\n",
      "Epoch [8/20]\n",
      "  Train Loss: 0.0230 | Train Acc: 0.9929\n",
      "  Test  Loss: 0.9084 | Test  Acc: 0.7797\n",
      "Epoch [9/20]\n",
      "  Train Loss: 0.0166 | Train Acc: 0.9944\n",
      "  Test  Loss: 0.8115 | Test  Acc: 0.8023\n",
      "Epoch [10/20]\n",
      "  Train Loss: 0.0197 | Train Acc: 0.9929\n",
      "  Test  Loss: 0.7005 | Test  Acc: 0.8362\n",
      "Epoch [11/20]\n",
      "  Train Loss: 0.0156 | Train Acc: 0.9958\n",
      "  Test  Loss: 0.6578 | Test  Acc: 0.8475\n",
      "Epoch [12/20]\n",
      "  Train Loss: 0.0224 | Train Acc: 0.9944\n",
      "  Test  Loss: 0.6483 | Test  Acc: 0.8418\n",
      "Epoch [13/20]\n",
      "  Train Loss: 0.0175 | Train Acc: 0.9958\n",
      "  Test  Loss: 0.6338 | Test  Acc: 0.8588\n",
      "Epoch [14/20]\n",
      "  Train Loss: 0.0144 | Train Acc: 0.9958\n",
      "  Test  Loss: 0.6115 | Test  Acc: 0.8757\n",
      "Epoch [15/20]\n",
      "  Train Loss: 0.0184 | Train Acc: 0.9944\n",
      "  Test  Loss: 0.6145 | Test  Acc: 0.8757\n",
      "Epoch [16/20]\n",
      "  Train Loss: 0.0112 | Train Acc: 0.9958\n",
      "  Test  Loss: 0.6099 | Test  Acc: 0.8701\n",
      "Epoch [17/20]\n",
      "  Train Loss: 0.0161 | Train Acc: 0.9944\n",
      "  Test  Loss: 0.5894 | Test  Acc: 0.8757\n",
      "Epoch [18/20]\n",
      "  Train Loss: 0.0113 | Train Acc: 0.9972\n",
      "  Test  Loss: 0.5785 | Test  Acc: 0.8757\n",
      "Epoch [19/20]\n",
      "  Train Loss: 0.0121 | Train Acc: 0.9958\n",
      "  Test  Loss: 0.5489 | Test  Acc: 0.8814\n",
      "Epoch [20/20]\n",
      "  Train Loss: 0.0125 | Train Acc: 0.9958\n",
      "  Test  Loss: 0.5320 | Test  Acc: 0.8814\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# ===============================\n",
    "# 1. 하이퍼파라미터 및 세팅\n",
    "# ===============================\n",
    "model_name = 'efficientnet-b0'\n",
    "num_classes = 6\n",
    "batch_size = 128\n",
    "epochs = 128\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ===============================\n",
    "# 2. 데이터셋 & 전처리\n",
    "# ===============================\n",
    "# 예: 데이터 증강을 좀 더 다양하게 사용 가능\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),    # 수평 뒤집기\n",
    "    transforms.RandomRotation(degrees=15),     # 회전\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset_path = \"C:/Users/IIALAB/Desktop/kdm/solar/kaggle/input/solar-panel-images/Faulty_solar_panel\"\n",
    "# 전체 데이터셋 (train_transform를 먼저 적용)\n",
    "full_dataset = datasets.ImageFolder(root=dataset_path, transform=train_transform)\n",
    "\n",
    "# 80%: Train / 20%: Test 로 분할\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# 검증 세트에는 test_transform 적용을 위해 transform 교체\n",
    "test_dataset.dataset.transform = test_transform\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ===============================\n",
    "# 3. 모델 초기화 (커스텀 모델)\n",
    "# ===============================\n",
    "#from custom_efficientnet import CustomEfficientNet  # (위 클래스가 들어있다고 가정)\n",
    "model = CustomEfficientNet(model_name=model_name,\n",
    "                           num_classes=num_classes,\n",
    "                           freeze_backbone=False).to(device)\n",
    "\n",
    "# ===============================\n",
    "# 4. 손실 함수 & 최적화 함수\n",
    "# ===============================\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# ===============================\n",
    "# 5. 학습 함수\n",
    "# ===============================\n",
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# ===============================\n",
    "# 6. 평가 함수\n",
    "# ===============================\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# ===============================\n",
    "# 7. 학습 루프\n",
    "# ===============================\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Test  Loss: {test_loss:.4f} | Test  Acc: {test_acc:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Epoch [1/128]\n",
      "  Train Loss: 1.0410 | Train Acc: 0.6201\n",
      "  Test  Loss: 1.6658 | Test  Acc: 0.4802\n",
      "Epoch [2/128]\n",
      "  Train Loss: 0.4192 | Train Acc: 0.8686\n",
      "  Test  Loss: 0.8904 | Test  Acc: 0.7062\n",
      "Epoch [3/128]\n",
      "  Train Loss: 0.2538 | Train Acc: 0.9237\n",
      "  Test  Loss: 0.7234 | Test  Acc: 0.7458\n",
      "Epoch [4/128]\n",
      "  Train Loss: 0.1541 | Train Acc: 0.9506\n",
      "  Test  Loss: 0.6977 | Test  Acc: 0.8079\n",
      "Epoch [5/128]\n",
      "  Train Loss: 0.2125 | Train Acc: 0.9407\n",
      "  Test  Loss: 0.5445 | Test  Acc: 0.8418\n",
      "Epoch [6/128]\n",
      "  Train Loss: 0.1500 | Train Acc: 0.9576\n",
      "  Test  Loss: 0.8973 | Test  Acc: 0.7684\n",
      "Epoch [7/128]\n",
      "  Train Loss: 0.1295 | Train Acc: 0.9689\n",
      "  Test  Loss: 0.5580 | Test  Acc: 0.8531\n",
      "Epoch [8/128]\n",
      "  Train Loss: 0.1073 | Train Acc: 0.9661\n",
      "  Test  Loss: 0.6404 | Test  Acc: 0.8475\n",
      "Epoch [9/128]\n",
      "  Train Loss: 0.1159 | Train Acc: 0.9647\n",
      "  Test  Loss: 0.5849 | Test  Acc: 0.8701\n",
      "Epoch [10/128]\n",
      "  Train Loss: 0.0691 | Train Acc: 0.9788\n",
      "  Test  Loss: 0.5354 | Test  Acc: 0.8701\n",
      "Epoch [11/128]\n",
      "  Train Loss: 0.0774 | Train Acc: 0.9831\n",
      "  Test  Loss: 0.5239 | Test  Acc: 0.8814\n",
      "Epoch [12/128]\n",
      "  Train Loss: 0.1027 | Train Acc: 0.9689\n",
      "  Test  Loss: 0.5498 | Test  Acc: 0.8644\n",
      "Epoch [13/128]\n",
      "  Train Loss: 0.1589 | Train Acc: 0.9534\n",
      "  Test  Loss: 0.7360 | Test  Acc: 0.8023\n",
      "Epoch [14/128]\n",
      "  Train Loss: 0.0984 | Train Acc: 0.9703\n",
      "  Test  Loss: 0.7393 | Test  Acc: 0.8418\n",
      "Epoch [15/128]\n",
      "  Train Loss: 0.1975 | Train Acc: 0.9492\n",
      "  Test  Loss: 1.0091 | Test  Acc: 0.7853\n",
      "Epoch [16/128]\n",
      "  Train Loss: 0.3044 | Train Acc: 0.9110\n",
      "  Test  Loss: 0.7814 | Test  Acc: 0.7966\n",
      "Epoch [17/128]\n",
      "  Train Loss: 0.1323 | Train Acc: 0.9605\n",
      "  Test  Loss: 0.6870 | Test  Acc: 0.8192\n",
      "Epoch [18/128]\n",
      "  Train Loss: 0.1844 | Train Acc: 0.9421\n",
      "  Test  Loss: 0.6827 | Test  Acc: 0.8249\n",
      "Epoch [19/128]\n",
      "  Train Loss: 0.1306 | Train Acc: 0.9647\n",
      "  Test  Loss: 0.5475 | Test  Acc: 0.8644\n",
      "Epoch [20/128]\n",
      "  Train Loss: 0.1526 | Train Acc: 0.9449\n",
      "  Test  Loss: 0.7235 | Test  Acc: 0.7910\n",
      "Epoch [21/128]\n",
      "  Train Loss: 0.1971 | Train Acc: 0.9548\n",
      "  Test  Loss: 0.6465 | Test  Acc: 0.8136\n",
      "Epoch [22/128]\n",
      "  Train Loss: 0.1611 | Train Acc: 0.9605\n",
      "  Test  Loss: 0.6778 | Test  Acc: 0.8475\n",
      "Epoch [23/128]\n",
      "  Train Loss: 0.1409 | Train Acc: 0.9633\n",
      "  Test  Loss: 0.8326 | Test  Acc: 0.8249\n",
      "Epoch [24/128]\n",
      "  Train Loss: 0.0784 | Train Acc: 0.9774\n",
      "  Test  Loss: 0.5948 | Test  Acc: 0.8644\n",
      "Epoch [25/128]\n",
      "  Train Loss: 0.1796 | Train Acc: 0.9506\n",
      "  Test  Loss: 0.7340 | Test  Acc: 0.7910\n",
      "Epoch [26/128]\n",
      "  Train Loss: 0.2463 | Train Acc: 0.9181\n",
      "  Test  Loss: 0.7005 | Test  Acc: 0.8192\n",
      "Epoch [27/128]\n",
      "  Train Loss: 0.2036 | Train Acc: 0.9379\n",
      "  Test  Loss: 0.7862 | Test  Acc: 0.8023\n",
      "Epoch [28/128]\n",
      "  Train Loss: 0.1855 | Train Acc: 0.9449\n",
      "  Test  Loss: 0.7165 | Test  Acc: 0.8192\n",
      "Epoch [29/128]\n",
      "  Train Loss: 0.1645 | Train Acc: 0.9449\n",
      "  Test  Loss: 0.6751 | Test  Acc: 0.8475\n",
      "Epoch [30/128]\n",
      "  Train Loss: 0.1874 | Train Acc: 0.9322\n",
      "  Test  Loss: 0.8010 | Test  Acc: 0.7966\n",
      "Epoch [31/128]\n",
      "  Train Loss: 0.1126 | Train Acc: 0.9647\n",
      "  Test  Loss: 0.7975 | Test  Acc: 0.8249\n",
      "Epoch [32/128]\n",
      "  Train Loss: 0.1440 | Train Acc: 0.9619\n",
      "  Test  Loss: 0.9394 | Test  Acc: 0.8079\n",
      "Epoch [33/128]\n",
      "  Train Loss: 0.1536 | Train Acc: 0.9492\n",
      "  Test  Loss: 0.7194 | Test  Acc: 0.8192\n",
      "Epoch [34/128]\n",
      "  Train Loss: 0.0973 | Train Acc: 0.9718\n",
      "  Test  Loss: 0.6806 | Test  Acc: 0.8249\n",
      "Epoch [35/128]\n",
      "  Train Loss: 0.1875 | Train Acc: 0.9449\n",
      "  Test  Loss: 0.6595 | Test  Acc: 0.8136\n",
      "Epoch [36/128]\n",
      "  Train Loss: 0.1926 | Train Acc: 0.9506\n",
      "  Test  Loss: 0.7963 | Test  Acc: 0.8418\n",
      "Epoch [37/128]\n",
      "  Train Loss: 0.1248 | Train Acc: 0.9619\n",
      "  Test  Loss: 1.1437 | Test  Acc: 0.7458\n",
      "Epoch [38/128]\n",
      "  Train Loss: 0.0792 | Train Acc: 0.9746\n",
      "  Test  Loss: 0.6583 | Test  Acc: 0.8305\n",
      "Epoch [39/128]\n",
      "  Train Loss: 0.0600 | Train Acc: 0.9873\n",
      "  Test  Loss: 0.5641 | Test  Acc: 0.8588\n",
      "Epoch [40/128]\n",
      "  Train Loss: 0.0456 | Train Acc: 0.9915\n",
      "  Test  Loss: 0.5454 | Test  Acc: 0.8701\n",
      "Epoch [41/128]\n",
      "  Train Loss: 0.1112 | Train Acc: 0.9774\n",
      "  Test  Loss: 0.6283 | Test  Acc: 0.8475\n",
      "Epoch [42/128]\n",
      "  Train Loss: 0.0828 | Train Acc: 0.9746\n",
      "  Test  Loss: 0.6049 | Test  Acc: 0.8475\n",
      "Epoch [43/128]\n",
      "  Train Loss: 0.0593 | Train Acc: 0.9859\n",
      "  Test  Loss: 0.5824 | Test  Acc: 0.8588\n",
      "Epoch [44/128]\n",
      "  Train Loss: 0.0222 | Train Acc: 0.9944\n",
      "  Test  Loss: 0.5440 | Test  Acc: 0.8870\n",
      "Epoch [45/128]\n",
      "  Train Loss: 0.0371 | Train Acc: 0.9901\n",
      "  Test  Loss: 0.5164 | Test  Acc: 0.8983\n",
      "Epoch [46/128]\n",
      "  Train Loss: 0.1060 | Train Acc: 0.9703\n",
      "  Test  Loss: 0.6260 | Test  Acc: 0.8475\n",
      "Epoch [47/128]\n",
      "  Train Loss: 0.0450 | Train Acc: 0.9901\n",
      "  Test  Loss: 0.5738 | Test  Acc: 0.8588\n",
      "Epoch [48/128]\n",
      "  Train Loss: 0.1450 | Train Acc: 0.9576\n",
      "  Test  Loss: 0.8243 | Test  Acc: 0.7966\n",
      "Epoch [49/128]\n",
      "  Train Loss: 0.1069 | Train Acc: 0.9703\n",
      "  Test  Loss: 0.6469 | Test  Acc: 0.8192\n",
      "Epoch [50/128]\n",
      "  Train Loss: 0.0894 | Train Acc: 0.9675\n",
      "  Test  Loss: 0.7088 | Test  Acc: 0.8362\n",
      "Epoch [51/128]\n",
      "  Train Loss: 0.2200 | Train Acc: 0.9379\n",
      "  Test  Loss: 0.6898 | Test  Acc: 0.8362\n",
      "Epoch [52/128]\n",
      "  Train Loss: 0.0924 | Train Acc: 0.9732\n",
      "  Test  Loss: 0.6070 | Test  Acc: 0.8588\n",
      "Epoch [53/128]\n",
      "  Train Loss: 0.0922 | Train Acc: 0.9746\n",
      "  Test  Loss: 0.7473 | Test  Acc: 0.8475\n",
      "Epoch [54/128]\n",
      "  Train Loss: 0.1111 | Train Acc: 0.9633\n",
      "  Test  Loss: 0.7176 | Test  Acc: 0.8531\n",
      "Epoch [55/128]\n",
      "  Train Loss: 0.0390 | Train Acc: 0.9901\n",
      "  Test  Loss: 0.6083 | Test  Acc: 0.8192\n",
      "Epoch [56/128]\n",
      "  Train Loss: 0.0371 | Train Acc: 0.9929\n",
      "  Test  Loss: 0.5126 | Test  Acc: 0.8814\n",
      "Epoch [57/128]\n",
      "  Train Loss: 0.1413 | Train Acc: 0.9746\n",
      "  Test  Loss: 0.6731 | Test  Acc: 0.8475\n",
      "Epoch [58/128]\n",
      "  Train Loss: 0.1097 | Train Acc: 0.9689\n",
      "  Test  Loss: 0.5961 | Test  Acc: 0.8475\n",
      "Epoch [59/128]\n",
      "  Train Loss: 0.0563 | Train Acc: 0.9845\n",
      "  Test  Loss: 0.4904 | Test  Acc: 0.8870\n",
      "Epoch [60/128]\n",
      "  Train Loss: 0.1484 | Train Acc: 0.9619\n",
      "  Test  Loss: 0.7372 | Test  Acc: 0.8362\n",
      "Epoch [61/128]\n",
      "  Train Loss: 0.1564 | Train Acc: 0.9548\n",
      "  Test  Loss: 0.6826 | Test  Acc: 0.8418\n",
      "Epoch [62/128]\n",
      "  Train Loss: 0.1041 | Train Acc: 0.9718\n",
      "  Test  Loss: 0.7205 | Test  Acc: 0.8192\n",
      "Epoch [63/128]\n",
      "  Train Loss: 0.0924 | Train Acc: 0.9746\n",
      "  Test  Loss: 0.7210 | Test  Acc: 0.8475\n",
      "Epoch [64/128]\n",
      "  Train Loss: 0.0665 | Train Acc: 0.9788\n",
      "  Test  Loss: 0.6981 | Test  Acc: 0.8362\n",
      "Epoch [65/128]\n",
      "  Train Loss: 0.1424 | Train Acc: 0.9633\n",
      "  Test  Loss: 0.6524 | Test  Acc: 0.8249\n",
      "Epoch [66/128]\n",
      "  Train Loss: 0.0951 | Train Acc: 0.9774\n",
      "  Test  Loss: 0.5076 | Test  Acc: 0.8757\n",
      "Epoch [67/128]\n",
      "  Train Loss: 0.1080 | Train Acc: 0.9633\n",
      "  Test  Loss: 0.5952 | Test  Acc: 0.8249\n",
      "Epoch [68/128]\n",
      "  Train Loss: 0.0671 | Train Acc: 0.9816\n",
      "  Test  Loss: 0.6162 | Test  Acc: 0.8418\n",
      "Epoch [69/128]\n",
      "  Train Loss: 0.0621 | Train Acc: 0.9859\n",
      "  Test  Loss: 0.5172 | Test  Acc: 0.8644\n",
      "Epoch [70/128]\n",
      "  Train Loss: 0.1329 | Train Acc: 0.9661\n",
      "  Test  Loss: 0.6522 | Test  Acc: 0.8531\n",
      "Epoch [71/128]\n",
      "  Train Loss: 0.1151 | Train Acc: 0.9732\n",
      "  Test  Loss: 0.7998 | Test  Acc: 0.8249\n",
      "Epoch [72/128]\n",
      "  Train Loss: 0.0865 | Train Acc: 0.9859\n",
      "  Test  Loss: 0.9430 | Test  Acc: 0.7966\n",
      "Epoch [73/128]\n",
      "  Train Loss: 0.1356 | Train Acc: 0.9534\n",
      "  Test  Loss: 0.8238 | Test  Acc: 0.8079\n",
      "Epoch [74/128]\n",
      "  Train Loss: 0.1266 | Train Acc: 0.9576\n",
      "  Test  Loss: 1.2022 | Test  Acc: 0.7288\n",
      "Epoch [75/128]\n",
      "  Train Loss: 0.1242 | Train Acc: 0.9661\n",
      "  Test  Loss: 0.6305 | Test  Acc: 0.8362\n",
      "Epoch [76/128]\n",
      "  Train Loss: 0.0896 | Train Acc: 0.9774\n",
      "  Test  Loss: 0.5679 | Test  Acc: 0.8418\n",
      "Epoch [77/128]\n",
      "  Train Loss: 0.0576 | Train Acc: 0.9859\n",
      "  Test  Loss: 0.5711 | Test  Acc: 0.8362\n",
      "Epoch [78/128]\n",
      "  Train Loss: 0.0343 | Train Acc: 0.9958\n",
      "  Test  Loss: 0.5769 | Test  Acc: 0.8644\n",
      "Epoch [79/128]\n",
      "  Train Loss: 0.0315 | Train Acc: 0.9901\n",
      "  Test  Loss: 0.5705 | Test  Acc: 0.8362\n",
      "Epoch [80/128]\n",
      "  Train Loss: 0.0399 | Train Acc: 0.9831\n",
      "  Test  Loss: 0.5321 | Test  Acc: 0.8757\n",
      "Epoch [81/128]\n",
      "  Train Loss: 0.0327 | Train Acc: 0.9887\n",
      "  Test  Loss: 0.5021 | Test  Acc: 0.8814\n",
      "Epoch [82/128]\n",
      "  Train Loss: 0.0348 | Train Acc: 0.9901\n",
      "  Test  Loss: 0.5682 | Test  Acc: 0.8757\n",
      "Epoch [83/128]\n",
      "  Train Loss: 0.1081 | Train Acc: 0.9788\n",
      "  Test  Loss: 0.8804 | Test  Acc: 0.8305\n",
      "Epoch [84/128]\n",
      "  Train Loss: 0.1141 | Train Acc: 0.9703\n",
      "  Test  Loss: 0.8025 | Test  Acc: 0.8305\n",
      "Epoch [85/128]\n",
      "  Train Loss: 0.1490 | Train Acc: 0.9605\n",
      "  Test  Loss: 0.9363 | Test  Acc: 0.8079\n",
      "Epoch [86/128]\n",
      "  Train Loss: 0.0897 | Train Acc: 0.9788\n",
      "  Test  Loss: 0.7896 | Test  Acc: 0.8362\n",
      "Epoch [87/128]\n",
      "  Train Loss: 0.1421 | Train Acc: 0.9675\n",
      "  Test  Loss: 0.7530 | Test  Acc: 0.8136\n",
      "Epoch [88/128]\n",
      "  Train Loss: 0.0761 | Train Acc: 0.9802\n",
      "  Test  Loss: 0.5674 | Test  Acc: 0.8418\n",
      "Epoch [89/128]\n",
      "  Train Loss: 0.1683 | Train Acc: 0.9605\n",
      "  Test  Loss: 0.9698 | Test  Acc: 0.7458\n",
      "Epoch [90/128]\n",
      "  Train Loss: 0.2611 | Train Acc: 0.9336\n",
      "  Test  Loss: 1.0515 | Test  Acc: 0.7401\n",
      "Epoch [91/128]\n",
      "  Train Loss: 0.0992 | Train Acc: 0.9703\n",
      "  Test  Loss: 0.8210 | Test  Acc: 0.8023\n",
      "Epoch [92/128]\n",
      "  Train Loss: 0.0886 | Train Acc: 0.9746\n",
      "  Test  Loss: 0.7688 | Test  Acc: 0.8305\n",
      "Epoch [93/128]\n",
      "  Train Loss: 0.0629 | Train Acc: 0.9816\n",
      "  Test  Loss: 0.6050 | Test  Acc: 0.8475\n",
      "Epoch [94/128]\n",
      "  Train Loss: 0.0640 | Train Acc: 0.9732\n",
      "  Test  Loss: 0.4655 | Test  Acc: 0.8814\n",
      "Epoch [95/128]\n",
      "  Train Loss: 0.0881 | Train Acc: 0.9774\n",
      "  Test  Loss: 0.4675 | Test  Acc: 0.8927\n",
      "Epoch [96/128]\n",
      "  Train Loss: 0.0695 | Train Acc: 0.9774\n",
      "  Test  Loss: 0.5531 | Test  Acc: 0.8531\n",
      "Epoch [97/128]\n",
      "  Train Loss: 0.0634 | Train Acc: 0.9788\n",
      "  Test  Loss: 0.5908 | Test  Acc: 0.8757\n",
      "Epoch [98/128]\n",
      "  Train Loss: 0.0317 | Train Acc: 0.9929\n",
      "  Test  Loss: 0.5406 | Test  Acc: 0.8701\n",
      "Epoch [99/128]\n",
      "  Train Loss: 0.0463 | Train Acc: 0.9873\n",
      "  Test  Loss: 0.5492 | Test  Acc: 0.8588\n",
      "Epoch [100/128]\n",
      "  Train Loss: 0.0526 | Train Acc: 0.9845\n",
      "  Test  Loss: 0.6449 | Test  Acc: 0.8249\n",
      "Epoch [101/128]\n",
      "  Train Loss: 0.1564 | Train Acc: 0.9534\n",
      "  Test  Loss: 0.9762 | Test  Acc: 0.7627\n",
      "Epoch [102/128]\n",
      "  Train Loss: 0.1274 | Train Acc: 0.9619\n",
      "  Test  Loss: 0.9969 | Test  Acc: 0.7684\n",
      "Epoch [103/128]\n",
      "  Train Loss: 0.1911 | Train Acc: 0.9477\n",
      "  Test  Loss: 1.0783 | Test  Acc: 0.7006\n",
      "Epoch [104/128]\n",
      "  Train Loss: 0.1049 | Train Acc: 0.9746\n",
      "  Test  Loss: 0.8029 | Test  Acc: 0.8079\n",
      "Epoch [105/128]\n",
      "  Train Loss: 0.1209 | Train Acc: 0.9590\n",
      "  Test  Loss: 1.1891 | Test  Acc: 0.7853\n",
      "Epoch [106/128]\n",
      "  Train Loss: 0.1381 | Train Acc: 0.9647\n",
      "  Test  Loss: 0.5943 | Test  Acc: 0.8305\n",
      "Epoch [107/128]\n",
      "  Train Loss: 0.0509 | Train Acc: 0.9901\n",
      "  Test  Loss: 0.6009 | Test  Acc: 0.8475\n",
      "Epoch [108/128]\n",
      "  Train Loss: 0.0627 | Train Acc: 0.9831\n",
      "  Test  Loss: 0.6682 | Test  Acc: 0.8136\n",
      "Epoch [109/128]\n",
      "  Train Loss: 0.0577 | Train Acc: 0.9788\n",
      "  Test  Loss: 0.6444 | Test  Acc: 0.8475\n",
      "Epoch [110/128]\n",
      "  Train Loss: 0.0335 | Train Acc: 0.9901\n",
      "  Test  Loss: 0.6399 | Test  Acc: 0.8531\n",
      "Epoch [111/128]\n",
      "  Train Loss: 0.0330 | Train Acc: 0.9929\n",
      "  Test  Loss: 0.7135 | Test  Acc: 0.8588\n",
      "Epoch [112/128]\n",
      "  Train Loss: 0.0311 | Train Acc: 0.9915\n",
      "  Test  Loss: 0.5952 | Test  Acc: 0.8588\n",
      "Epoch [113/128]\n",
      "  Train Loss: 0.0593 | Train Acc: 0.9859\n",
      "  Test  Loss: 0.6559 | Test  Acc: 0.8418\n",
      "Epoch [114/128]\n",
      "  Train Loss: 0.0387 | Train Acc: 0.9873\n",
      "  Test  Loss: 0.5417 | Test  Acc: 0.8814\n",
      "Epoch [115/128]\n",
      "  Train Loss: 0.1066 | Train Acc: 0.9746\n",
      "  Test  Loss: 0.9557 | Test  Acc: 0.7853\n",
      "Epoch [116/128]\n",
      "  Train Loss: 0.0562 | Train Acc: 0.9831\n",
      "  Test  Loss: 0.5079 | Test  Acc: 0.8983\n",
      "Epoch [117/128]\n",
      "  Train Loss: 0.1486 | Train Acc: 0.9548\n",
      "  Test  Loss: 0.7059 | Test  Acc: 0.8418\n",
      "Epoch [118/128]\n",
      "  Train Loss: 0.0931 | Train Acc: 0.9774\n",
      "  Test  Loss: 0.8175 | Test  Acc: 0.8136\n",
      "Epoch [119/128]\n",
      "  Train Loss: 0.0646 | Train Acc: 0.9760\n",
      "  Test  Loss: 0.8836 | Test  Acc: 0.8023\n",
      "Epoch [120/128]\n",
      "  Train Loss: 0.0438 | Train Acc: 0.9873\n",
      "  Test  Loss: 1.0324 | Test  Acc: 0.8023\n",
      "Epoch [121/128]\n",
      "  Train Loss: 0.0695 | Train Acc: 0.9831\n",
      "  Test  Loss: 0.8036 | Test  Acc: 0.8136\n",
      "Epoch [122/128]\n",
      "  Train Loss: 0.0494 | Train Acc: 0.9859\n",
      "  Test  Loss: 0.6542 | Test  Acc: 0.8305\n",
      "Epoch [123/128]\n",
      "  Train Loss: 0.1026 | Train Acc: 0.9703\n",
      "  Test  Loss: 0.6925 | Test  Acc: 0.8192\n",
      "Epoch [124/128]\n",
      "  Train Loss: 0.1157 | Train Acc: 0.9718\n",
      "  Test  Loss: 0.7960 | Test  Acc: 0.8249\n",
      "Epoch [125/128]\n",
      "  Train Loss: 0.0594 | Train Acc: 0.9774\n",
      "  Test  Loss: 0.7154 | Test  Acc: 0.8475\n",
      "Epoch [126/128]\n",
      "  Train Loss: 0.0616 | Train Acc: 0.9859\n",
      "  Test  Loss: 0.6078 | Test  Acc: 0.8701\n",
      "Epoch [127/128]\n",
      "  Train Loss: 0.0540 | Train Acc: 0.9831\n",
      "  Test  Loss: 0.8362 | Test  Acc: 0.7910\n",
      "Epoch [128/128]\n",
      "  Train Loss: 0.1057 | Train Acc: 0.9647\n",
      "  Test  Loss: 0.5704 | Test  Acc: 0.8870\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# Early Stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# CustomEfficientNet 클래스 정의\n",
    "class CustomEfficientNet(nn.Module):\n",
    "    def __init__(self, model_name=\"efficientnet-b0\", num_classes=6, freeze_backbone=False):\n",
    "        super(CustomEfficientNet, self).__init__()\n",
    "        self.efficientnet = EfficientNet.from_pretrained(model_name)\n",
    "        if freeze_backbone:\n",
    "            for param in self.efficientnet.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        in_features = self.efficientnet._fc.in_features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        self.efficientnet._fc = self.classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.efficientnet(x)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "model_name = 'efficientnet-b0'\n",
    "num_classes = 6\n",
    "batch_size = 32\n",
    "epochs = 128\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 데이터 전처리 및 로드\n",
    "image_size = EfficientNet.get_image_size(model_name)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "dataset_path = 'C:/Users/IIALAB/Desktop/kdm/solar/kaggle/input/solar-panel-images/Faulty_solar_panel'\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 모델 초기화\n",
    "model = CustomEfficientNet(model_name=model_name, num_classes=num_classes, freeze_backbone=False).to(device)\n",
    "\n",
    "# 손실 함수 및 최적화기 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 학습 함수\n",
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "# 학습 루프\n",
    "best_test_acc = 0.0  # 최고 Test Accuracy 저장\n",
    "best_epoch = 0       # 최고 Accuracy를 기록한 Epoch\n",
    "early_stopping = EarlyStopping(patience=10)  # Early Stopping 설정\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    # 최고 Test Accuracy 갱신\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_epoch = epoch + 1  # Epoch 번호 저장\n",
    "\n",
    "    # Early Stopping 체크\n",
    "    early_stopping(test_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "    # Epoch 결과 출력\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Test  Loss: {test_loss:.4f}  | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "# 최고 Test Accuracy 출력\n",
    "print(f\"\\nTraining complete! Best Test Accuracy: {best_test_acc:.4f} at Epoch {best_epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CustomEfficientNet' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 83\u001b[0m\n\u001b[0;32m     80\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     81\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m())\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# 학습 및 평가 함수\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(model, loader, criterion, optimizer):\n",
      "File \u001b[1;32mc:\\Users\\IIALAB\\anaconda3\\envs\\solar\\lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CustomEfficientNet' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# Early Stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# 모델 클래스 정의 (CustomEfficientNet)\n",
    "class CustomEfficientNet(nn.Module):\n",
    "    def __init__(self, model_name=\"efficientnet-b0\", num_classes=6, freeze_backbone=False):\n",
    "        super(CustomEfficientNet, self).__init__()\n",
    "        self.backbone = EfficientNet.from_pretrained(model_name)\n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        in_features = self.backbone._fc.in_features\n",
    "        self.backbone._fc = nn.Identity()\n",
    "        self.extra_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_features, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone.extract_features(x)\n",
    "        x = self.extra_conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# 하이퍼파라미터\n",
    "model_name = 'efficientnet-b0'\n",
    "num_classes = 6\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 데이터 전처리 & 로더\n",
    "image_size = EfficientNet.get_image_size(model_name)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "dataset_path = 'C:/Users/IIALAB/Desktop/kdm/solar/kaggle/input/solar-panel-images/Faulty_solar_panel'\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 모델 및 손실 함수, 최적화기\n",
    "model = CustomEfficientNet(model_name=model_name, num_classes=num_classes, freeze_backbone=False).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 학습 및 평가 함수\n",
    "def train(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, pred = outputs.max(1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return running_loss/total, correct/total\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, pred = outputs.max(1)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return running_loss/total, correct/total\n",
    "\n",
    "# 학습 루프\n",
    "best_test_acc = 0.0\n",
    "best_epoch = 0\n",
    "early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_epoch = epoch + 1\n",
    "\n",
    "    early_stopping(test_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{epochs}] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}  | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nTraining complete! Best Test Accuracy: {best_test_acc:.4f} at Epoch {best_epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         ZeroPad2d-1          [-1, 3, 225, 225]               0\n",
      "Conv2dStaticSamePadding-2         [-1, 32, 112, 112]             864\n",
      "       BatchNorm2d-3         [-1, 32, 112, 112]              64\n",
      "MemoryEfficientSwish-4         [-1, 32, 112, 112]               0\n",
      "         ZeroPad2d-5         [-1, 32, 114, 114]               0\n",
      "Conv2dStaticSamePadding-6         [-1, 32, 112, 112]             288\n",
      "       BatchNorm2d-7         [-1, 32, 112, 112]              64\n",
      "MemoryEfficientSwish-8         [-1, 32, 112, 112]               0\n",
      "          Identity-9             [-1, 32, 1, 1]               0\n",
      "Conv2dStaticSamePadding-10              [-1, 8, 1, 1]             264\n",
      "MemoryEfficientSwish-11              [-1, 8, 1, 1]               0\n",
      "         Identity-12              [-1, 8, 1, 1]               0\n",
      "Conv2dStaticSamePadding-13             [-1, 32, 1, 1]             288\n",
      "         Identity-14         [-1, 32, 112, 112]               0\n",
      "Conv2dStaticSamePadding-15         [-1, 16, 112, 112]             512\n",
      "      BatchNorm2d-16         [-1, 16, 112, 112]              32\n",
      "      MBConvBlock-17         [-1, 16, 112, 112]               0\n",
      "         Identity-18         [-1, 16, 112, 112]               0\n",
      "Conv2dStaticSamePadding-19         [-1, 96, 112, 112]           1,536\n",
      "      BatchNorm2d-20         [-1, 96, 112, 112]             192\n",
      "MemoryEfficientSwish-21         [-1, 96, 112, 112]               0\n",
      "        ZeroPad2d-22         [-1, 96, 113, 113]               0\n",
      "Conv2dStaticSamePadding-23           [-1, 96, 56, 56]             864\n",
      "      BatchNorm2d-24           [-1, 96, 56, 56]             192\n",
      "MemoryEfficientSwish-25           [-1, 96, 56, 56]               0\n",
      "         Identity-26             [-1, 96, 1, 1]               0\n",
      "Conv2dStaticSamePadding-27              [-1, 4, 1, 1]             388\n",
      "MemoryEfficientSwish-28              [-1, 4, 1, 1]               0\n",
      "         Identity-29              [-1, 4, 1, 1]               0\n",
      "Conv2dStaticSamePadding-30             [-1, 96, 1, 1]             480\n",
      "         Identity-31           [-1, 96, 56, 56]               0\n",
      "Conv2dStaticSamePadding-32           [-1, 24, 56, 56]           2,304\n",
      "      BatchNorm2d-33           [-1, 24, 56, 56]              48\n",
      "      MBConvBlock-34           [-1, 24, 56, 56]               0\n",
      "         Identity-35           [-1, 24, 56, 56]               0\n",
      "Conv2dStaticSamePadding-36          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-37          [-1, 144, 56, 56]             288\n",
      "MemoryEfficientSwish-38          [-1, 144, 56, 56]               0\n",
      "        ZeroPad2d-39          [-1, 144, 58, 58]               0\n",
      "Conv2dStaticSamePadding-40          [-1, 144, 56, 56]           1,296\n",
      "      BatchNorm2d-41          [-1, 144, 56, 56]             288\n",
      "MemoryEfficientSwish-42          [-1, 144, 56, 56]               0\n",
      "         Identity-43            [-1, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-44              [-1, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-45              [-1, 6, 1, 1]               0\n",
      "         Identity-46              [-1, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-47            [-1, 144, 1, 1]           1,008\n",
      "         Identity-48          [-1, 144, 56, 56]               0\n",
      "Conv2dStaticSamePadding-49           [-1, 24, 56, 56]           3,456\n",
      "      BatchNorm2d-50           [-1, 24, 56, 56]              48\n",
      "      MBConvBlock-51           [-1, 24, 56, 56]               0\n",
      "         Identity-52           [-1, 24, 56, 56]               0\n",
      "Conv2dStaticSamePadding-53          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-54          [-1, 144, 56, 56]             288\n",
      "MemoryEfficientSwish-55          [-1, 144, 56, 56]               0\n",
      "        ZeroPad2d-56          [-1, 144, 59, 59]               0\n",
      "Conv2dStaticSamePadding-57          [-1, 144, 28, 28]           3,600\n",
      "      BatchNorm2d-58          [-1, 144, 28, 28]             288\n",
      "MemoryEfficientSwish-59          [-1, 144, 28, 28]               0\n",
      "         Identity-60            [-1, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-61              [-1, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-62              [-1, 6, 1, 1]               0\n",
      "         Identity-63              [-1, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-64            [-1, 144, 1, 1]           1,008\n",
      "         Identity-65          [-1, 144, 28, 28]               0\n",
      "Conv2dStaticSamePadding-66           [-1, 40, 28, 28]           5,760\n",
      "      BatchNorm2d-67           [-1, 40, 28, 28]              80\n",
      "      MBConvBlock-68           [-1, 40, 28, 28]               0\n",
      "         Identity-69           [-1, 40, 28, 28]               0\n",
      "Conv2dStaticSamePadding-70          [-1, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-71          [-1, 240, 28, 28]             480\n",
      "MemoryEfficientSwish-72          [-1, 240, 28, 28]               0\n",
      "        ZeroPad2d-73          [-1, 240, 32, 32]               0\n",
      "Conv2dStaticSamePadding-74          [-1, 240, 28, 28]           6,000\n",
      "      BatchNorm2d-75          [-1, 240, 28, 28]             480\n",
      "MemoryEfficientSwish-76          [-1, 240, 28, 28]               0\n",
      "         Identity-77            [-1, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-78             [-1, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-79             [-1, 10, 1, 1]               0\n",
      "         Identity-80             [-1, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-81            [-1, 240, 1, 1]           2,640\n",
      "         Identity-82          [-1, 240, 28, 28]               0\n",
      "Conv2dStaticSamePadding-83           [-1, 40, 28, 28]           9,600\n",
      "      BatchNorm2d-84           [-1, 40, 28, 28]              80\n",
      "      MBConvBlock-85           [-1, 40, 28, 28]               0\n",
      "         Identity-86           [-1, 40, 28, 28]               0\n",
      "Conv2dStaticSamePadding-87          [-1, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-88          [-1, 240, 28, 28]             480\n",
      "MemoryEfficientSwish-89          [-1, 240, 28, 28]               0\n",
      "        ZeroPad2d-90          [-1, 240, 29, 29]               0\n",
      "Conv2dStaticSamePadding-91          [-1, 240, 14, 14]           2,160\n",
      "      BatchNorm2d-92          [-1, 240, 14, 14]             480\n",
      "MemoryEfficientSwish-93          [-1, 240, 14, 14]               0\n",
      "         Identity-94            [-1, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-95             [-1, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-96             [-1, 10, 1, 1]               0\n",
      "         Identity-97             [-1, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-98            [-1, 240, 1, 1]           2,640\n",
      "         Identity-99          [-1, 240, 14, 14]               0\n",
      "Conv2dStaticSamePadding-100           [-1, 80, 14, 14]          19,200\n",
      "     BatchNorm2d-101           [-1, 80, 14, 14]             160\n",
      "     MBConvBlock-102           [-1, 80, 14, 14]               0\n",
      "        Identity-103           [-1, 80, 14, 14]               0\n",
      "Conv2dStaticSamePadding-104          [-1, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-105          [-1, 480, 14, 14]             960\n",
      "MemoryEfficientSwish-106          [-1, 480, 14, 14]               0\n",
      "       ZeroPad2d-107          [-1, 480, 16, 16]               0\n",
      "Conv2dStaticSamePadding-108          [-1, 480, 14, 14]           4,320\n",
      "     BatchNorm2d-109          [-1, 480, 14, 14]             960\n",
      "MemoryEfficientSwish-110          [-1, 480, 14, 14]               0\n",
      "        Identity-111            [-1, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-112             [-1, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-113             [-1, 20, 1, 1]               0\n",
      "        Identity-114             [-1, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-115            [-1, 480, 1, 1]          10,080\n",
      "        Identity-116          [-1, 480, 14, 14]               0\n",
      "Conv2dStaticSamePadding-117           [-1, 80, 14, 14]          38,400\n",
      "     BatchNorm2d-118           [-1, 80, 14, 14]             160\n",
      "     MBConvBlock-119           [-1, 80, 14, 14]               0\n",
      "        Identity-120           [-1, 80, 14, 14]               0\n",
      "Conv2dStaticSamePadding-121          [-1, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-122          [-1, 480, 14, 14]             960\n",
      "MemoryEfficientSwish-123          [-1, 480, 14, 14]               0\n",
      "       ZeroPad2d-124          [-1, 480, 16, 16]               0\n",
      "Conv2dStaticSamePadding-125          [-1, 480, 14, 14]           4,320\n",
      "     BatchNorm2d-126          [-1, 480, 14, 14]             960\n",
      "MemoryEfficientSwish-127          [-1, 480, 14, 14]               0\n",
      "        Identity-128            [-1, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-129             [-1, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-130             [-1, 20, 1, 1]               0\n",
      "        Identity-131             [-1, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-132            [-1, 480, 1, 1]          10,080\n",
      "        Identity-133          [-1, 480, 14, 14]               0\n",
      "Conv2dStaticSamePadding-134           [-1, 80, 14, 14]          38,400\n",
      "     BatchNorm2d-135           [-1, 80, 14, 14]             160\n",
      "     MBConvBlock-136           [-1, 80, 14, 14]               0\n",
      "        Identity-137           [-1, 80, 14, 14]               0\n",
      "Conv2dStaticSamePadding-138          [-1, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-139          [-1, 480, 14, 14]             960\n",
      "MemoryEfficientSwish-140          [-1, 480, 14, 14]               0\n",
      "       ZeroPad2d-141          [-1, 480, 18, 18]               0\n",
      "Conv2dStaticSamePadding-142          [-1, 480, 14, 14]          12,000\n",
      "     BatchNorm2d-143          [-1, 480, 14, 14]             960\n",
      "MemoryEfficientSwish-144          [-1, 480, 14, 14]               0\n",
      "        Identity-145            [-1, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-146             [-1, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-147             [-1, 20, 1, 1]               0\n",
      "        Identity-148             [-1, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-149            [-1, 480, 1, 1]          10,080\n",
      "        Identity-150          [-1, 480, 14, 14]               0\n",
      "Conv2dStaticSamePadding-151          [-1, 112, 14, 14]          53,760\n",
      "     BatchNorm2d-152          [-1, 112, 14, 14]             224\n",
      "     MBConvBlock-153          [-1, 112, 14, 14]               0\n",
      "        Identity-154          [-1, 112, 14, 14]               0\n",
      "Conv2dStaticSamePadding-155          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-156          [-1, 672, 14, 14]           1,344\n",
      "MemoryEfficientSwish-157          [-1, 672, 14, 14]               0\n",
      "       ZeroPad2d-158          [-1, 672, 18, 18]               0\n",
      "Conv2dStaticSamePadding-159          [-1, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-160          [-1, 672, 14, 14]           1,344\n",
      "MemoryEfficientSwish-161          [-1, 672, 14, 14]               0\n",
      "        Identity-162            [-1, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-163             [-1, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-164             [-1, 28, 1, 1]               0\n",
      "        Identity-165             [-1, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-166            [-1, 672, 1, 1]          19,488\n",
      "        Identity-167          [-1, 672, 14, 14]               0\n",
      "Conv2dStaticSamePadding-168          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-169          [-1, 112, 14, 14]             224\n",
      "     MBConvBlock-170          [-1, 112, 14, 14]               0\n",
      "        Identity-171          [-1, 112, 14, 14]               0\n",
      "Conv2dStaticSamePadding-172          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-173          [-1, 672, 14, 14]           1,344\n",
      "MemoryEfficientSwish-174          [-1, 672, 14, 14]               0\n",
      "       ZeroPad2d-175          [-1, 672, 18, 18]               0\n",
      "Conv2dStaticSamePadding-176          [-1, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-177          [-1, 672, 14, 14]           1,344\n",
      "MemoryEfficientSwish-178          [-1, 672, 14, 14]               0\n",
      "        Identity-179            [-1, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-180             [-1, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-181             [-1, 28, 1, 1]               0\n",
      "        Identity-182             [-1, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-183            [-1, 672, 1, 1]          19,488\n",
      "        Identity-184          [-1, 672, 14, 14]               0\n",
      "Conv2dStaticSamePadding-185          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-186          [-1, 112, 14, 14]             224\n",
      "     MBConvBlock-187          [-1, 112, 14, 14]               0\n",
      "        Identity-188          [-1, 112, 14, 14]               0\n",
      "Conv2dStaticSamePadding-189          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-190          [-1, 672, 14, 14]           1,344\n",
      "MemoryEfficientSwish-191          [-1, 672, 14, 14]               0\n",
      "       ZeroPad2d-192          [-1, 672, 17, 17]               0\n",
      "Conv2dStaticSamePadding-193            [-1, 672, 7, 7]          16,800\n",
      "     BatchNorm2d-194            [-1, 672, 7, 7]           1,344\n",
      "MemoryEfficientSwish-195            [-1, 672, 7, 7]               0\n",
      "        Identity-196            [-1, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-197             [-1, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-198             [-1, 28, 1, 1]               0\n",
      "        Identity-199             [-1, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-200            [-1, 672, 1, 1]          19,488\n",
      "        Identity-201            [-1, 672, 7, 7]               0\n",
      "Conv2dStaticSamePadding-202            [-1, 192, 7, 7]         129,024\n",
      "     BatchNorm2d-203            [-1, 192, 7, 7]             384\n",
      "     MBConvBlock-204            [-1, 192, 7, 7]               0\n",
      "        Identity-205            [-1, 192, 7, 7]               0\n",
      "Conv2dStaticSamePadding-206           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-207           [-1, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-208           [-1, 1152, 7, 7]               0\n",
      "       ZeroPad2d-209         [-1, 1152, 11, 11]               0\n",
      "Conv2dStaticSamePadding-210           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-211           [-1, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-212           [-1, 1152, 7, 7]               0\n",
      "        Identity-213           [-1, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-214             [-1, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-215             [-1, 48, 1, 1]               0\n",
      "        Identity-216             [-1, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-217           [-1, 1152, 1, 1]          56,448\n",
      "        Identity-218           [-1, 1152, 7, 7]               0\n",
      "Conv2dStaticSamePadding-219            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-220            [-1, 192, 7, 7]             384\n",
      "     MBConvBlock-221            [-1, 192, 7, 7]               0\n",
      "        Identity-222            [-1, 192, 7, 7]               0\n",
      "Conv2dStaticSamePadding-223           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-224           [-1, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-225           [-1, 1152, 7, 7]               0\n",
      "       ZeroPad2d-226         [-1, 1152, 11, 11]               0\n",
      "Conv2dStaticSamePadding-227           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-228           [-1, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-229           [-1, 1152, 7, 7]               0\n",
      "        Identity-230           [-1, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-231             [-1, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-232             [-1, 48, 1, 1]               0\n",
      "        Identity-233             [-1, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-234           [-1, 1152, 1, 1]          56,448\n",
      "        Identity-235           [-1, 1152, 7, 7]               0\n",
      "Conv2dStaticSamePadding-236            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-237            [-1, 192, 7, 7]             384\n",
      "     MBConvBlock-238            [-1, 192, 7, 7]               0\n",
      "        Identity-239            [-1, 192, 7, 7]               0\n",
      "Conv2dStaticSamePadding-240           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-241           [-1, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-242           [-1, 1152, 7, 7]               0\n",
      "       ZeroPad2d-243         [-1, 1152, 11, 11]               0\n",
      "Conv2dStaticSamePadding-244           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-245           [-1, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-246           [-1, 1152, 7, 7]               0\n",
      "        Identity-247           [-1, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-248             [-1, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-249             [-1, 48, 1, 1]               0\n",
      "        Identity-250             [-1, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-251           [-1, 1152, 1, 1]          56,448\n",
      "        Identity-252           [-1, 1152, 7, 7]               0\n",
      "Conv2dStaticSamePadding-253            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-254            [-1, 192, 7, 7]             384\n",
      "     MBConvBlock-255            [-1, 192, 7, 7]               0\n",
      "        Identity-256            [-1, 192, 7, 7]               0\n",
      "Conv2dStaticSamePadding-257           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-258           [-1, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-259           [-1, 1152, 7, 7]               0\n",
      "       ZeroPad2d-260           [-1, 1152, 9, 9]               0\n",
      "Conv2dStaticSamePadding-261           [-1, 1152, 7, 7]          10,368\n",
      "     BatchNorm2d-262           [-1, 1152, 7, 7]           2,304\n",
      "MemoryEfficientSwish-263           [-1, 1152, 7, 7]               0\n",
      "        Identity-264           [-1, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-265             [-1, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-266             [-1, 48, 1, 1]               0\n",
      "        Identity-267             [-1, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-268           [-1, 1152, 1, 1]          56,448\n",
      "        Identity-269           [-1, 1152, 7, 7]               0\n",
      "Conv2dStaticSamePadding-270            [-1, 320, 7, 7]         368,640\n",
      "     BatchNorm2d-271            [-1, 320, 7, 7]             640\n",
      "     MBConvBlock-272            [-1, 320, 7, 7]               0\n",
      "        Identity-273            [-1, 320, 7, 7]               0\n",
      "Conv2dStaticSamePadding-274           [-1, 1280, 7, 7]         409,600\n",
      "     BatchNorm2d-275           [-1, 1280, 7, 7]           2,560\n",
      "MemoryEfficientSwish-276           [-1, 1280, 7, 7]               0\n",
      "          Conv2d-277            [-1, 512, 7, 7]       5,898,752\n",
      "            ReLU-278            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-279            [-1, 512, 1, 1]               0\n",
      "          Linear-280                  [-1, 256]         131,328\n",
      "            ReLU-281                  [-1, 256]               0\n",
      "         Dropout-282                  [-1, 256]               0\n",
      "          Linear-283                    [-1, 6]           1,542\n",
      "================================================================\n",
      "Total params: 10,039,170\n",
      "Trainable params: 10,039,170\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 213.16\n",
      "Params size (MB): 38.30\n",
      "Estimated Total Size (MB): 252.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# 모델 초기화\n",
    "model = CustomEfficientNet(model_name=model_name, num_classes=num_classes, freeze_backbone=False).to(device)\n",
    "\n",
    "# 모델 요약 출력\n",
    "summary(model, input_size=(3, image_size, image_size))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "[Epoch 1/50] Train Loss: 1.0384 | Train Acc: 0.6172\n",
      "  Test Loss: 1.3820  | Test Acc: 0.5706\n",
      "[Epoch 2/50] Train Loss: 0.4306 | Train Acc: 0.8630\n",
      "  Test Loss: 1.2499  | Test Acc: 0.7232\n",
      "[Epoch 3/50] Train Loss: 0.2741 | Train Acc: 0.9153\n",
      "  Test Loss: 1.6297  | Test Acc: 0.7514\n",
      "[Epoch 4/50] Train Loss: 0.3404 | Train Acc: 0.8983\n",
      "  Test Loss: 0.7878  | Test Acc: 0.7571\n",
      "[Epoch 5/50] Train Loss: 0.2869 | Train Acc: 0.9011\n",
      "  Test Loss: 0.8492  | Test Acc: 0.8644\n",
      "[Epoch 6/50] Train Loss: 0.2274 | Train Acc: 0.9308\n",
      "  Test Loss: 0.9488  | Test Acc: 0.8475\n",
      "[Epoch 7/50] Train Loss: 0.1981 | Train Acc: 0.9336\n",
      "  Test Loss: 1.2091  | Test Acc: 0.7797\n",
      "[Epoch 8/50] Train Loss: 0.2540 | Train Acc: 0.9463\n",
      "  Test Loss: 0.9144  | Test Acc: 0.8305\n",
      "[Epoch 9/50] Train Loss: 0.2039 | Train Acc: 0.9576\n",
      "  Test Loss: 0.9671  | Test Acc: 0.8418\n",
      "[Epoch 10/50] Train Loss: 0.1771 | Train Acc: 0.9534\n",
      "  Test Loss: 0.6136  | Test Acc: 0.8588\n",
      "[Epoch 11/50] Train Loss: 0.1927 | Train Acc: 0.9463\n",
      "  Test Loss: 1.0689  | Test Acc: 0.8023\n",
      "[Epoch 12/50] Train Loss: 0.2250 | Train Acc: 0.9322\n",
      "  Test Loss: 0.9160  | Test Acc: 0.8305\n",
      "[Epoch 13/50] Train Loss: 0.2407 | Train Acc: 0.9322\n",
      "  Test Loss: 0.9089  | Test Acc: 0.8927\n",
      "[Epoch 14/50] Train Loss: 0.1643 | Train Acc: 0.9463\n",
      "  Test Loss: 0.6432  | Test Acc: 0.8475\n",
      "[Epoch 15/50] Train Loss: 0.1197 | Train Acc: 0.9760\n",
      "  Test Loss: 0.5547  | Test Acc: 0.8757\n",
      "[Epoch 16/50] Train Loss: 0.0862 | Train Acc: 0.9746\n",
      "  Test Loss: 1.3827  | Test Acc: 0.8249\n",
      "[Epoch 17/50] Train Loss: 0.1600 | Train Acc: 0.9576\n",
      "  Test Loss: 1.0609  | Test Acc: 0.7910\n",
      "[Epoch 18/50] Train Loss: 0.1564 | Train Acc: 0.9534\n",
      "  Test Loss: 1.2211  | Test Acc: 0.8079\n",
      "[Epoch 19/50] Train Loss: 0.1738 | Train Acc: 0.9590\n",
      "  Test Loss: 0.7511  | Test Acc: 0.8531\n",
      "[Epoch 20/50] Train Loss: 0.3411 | Train Acc: 0.9167\n",
      "  Test Loss: 1.1813  | Test Acc: 0.7797\n",
      "[Epoch 21/50] Train Loss: 0.1354 | Train Acc: 0.9689\n",
      "  Test Loss: 0.9171  | Test Acc: 0.8701\n",
      "[Epoch 22/50] Train Loss: 0.0924 | Train Acc: 0.9718\n",
      "  Test Loss: 1.1258  | Test Acc: 0.8475\n",
      "[Epoch 23/50] Train Loss: 0.0638 | Train Acc: 0.9802\n",
      "  Test Loss: 0.9247  | Test Acc: 0.8588\n",
      "[Epoch 24/50] Train Loss: 0.0680 | Train Acc: 0.9816\n",
      "  Test Loss: 0.8495  | Test Acc: 0.8701\n",
      "Early stopping triggered\n",
      "\n",
      "Training complete! Best Test Accuracy: 0.8927 at Epoch 13\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "seed = 2021\n",
    "deterministic = True\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "if deterministic:\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\ttorch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Early Stopping 클래스\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# 모델 클래스 정의 (CustomEfficientNet)\n",
    "class CustomEfficientNet(nn.Module):\n",
    "    def __init__(self, model_name=\"efficientnet-b0\", num_classes=6, freeze_backbone=False):\n",
    "        super(CustomEfficientNet, self).__init__()\n",
    "        self.backbone = EfficientNet.from_pretrained(model_name)\n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        in_features = self.backbone._fc.in_features\n",
    "        self.backbone._fc = nn.Identity()\n",
    "        self.extra_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_features, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone.extract_features(x)\n",
    "        x = self.extra_conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# 하이퍼파라미터\n",
    "model_name = 'efficientnet-b0'\n",
    "num_classes = 6\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 데이터 전처리 & 로더\n",
    "image_size = EfficientNet.get_image_size(model_name)\n",
    "\n",
    "dataset_path = 'C:/Users/IIALAB/Desktop/kdm/solar/kaggle/input/solar-panel-images/Faulty_solar_panel'\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 모델 및 손실 함수, 최적화기\n",
    "model = CustomEfficientNet(model_name=model_name, num_classes=num_classes, freeze_backbone=False).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 학습 및 평가 함수\n",
    "def train(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, pred = outputs.max(1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return running_loss/total, correct/total\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, pred = outputs.max(1)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return running_loss/total, correct/total\n",
    "\n",
    "# 학습 루프\n",
    "best_test_acc = 0.0\n",
    "best_epoch = 0\n",
    "early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        best_epoch = epoch + 1\n",
    "\n",
    "    early_stopping(test_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{epochs}] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}  | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nTraining complete! Best Test Accuracy: {best_test_acc:.4f} at Epoch {best_epoch}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
